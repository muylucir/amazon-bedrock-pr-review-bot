import json
import os
from typing import Dict, List, Any, Union
from dataclasses import dataclass
from collections import defaultdict
import boto3
from datetime import datetime

@dataclass
class ReviewSummary:
    total_files: int
    total_primary_files: int
    total_reference_files: int
    total_issues: int
    severity_counts: Dict[str, int]
    category_counts: Dict[str, int]
    critical_issues: List[Dict[str, Any]]
    major_issues: List[Dict[str, Any]]
    suggestions_by_file: Dict[str, List[Dict[str, Any]]]
    reference_context: Dict[str, List[str]]
    # ë³€ê²½ì‚¬í•­ ìš”ì•½
    functional_changes: List[str]
    architectural_changes: List[str]
    technical_improvements: List[str]

class ResultAggregator:
    def __init__(self, event_data: Dict[str, Any]):
        self.ssm = boto3.client('ssm')
        self.event_data = event_data
        self.chunk_results = self._extract_chunk_results()
        self.pr_details = self._extract_pr_details()
        self.secrets = boto3.client('secretsmanager')
        self.config = self._load_config()

    def _load_config(self) -> Dict[str, Any]:
        """Parameter Storeì—ì„œ ì„¤ì • ë¡œë“œ"""
        config = {}
        try:
            # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
            response = self.ssm.get_parameters_by_path(
                Path='/pr-reviewer/config/',
                Recursive=True,
                WithDecryption=True
            )
            
            for param in response['Parameters']:
                # íŒŒë¼ë¯¸í„° ì´ë¦„ì—ì„œ ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                name = param['Name'].split('/')[-1]
                config[name] = param['Value']
                   
        except Exception as e:
            print(f"Error loading config: {e}")
            raise

        return config

    def _extract_pr_details(self) -> Dict[str, Any]:
        """PR ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            if isinstance(self.event_data, list) and self.event_data:
                # ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼
                for chunk in self.event_data:
                    if isinstance(chunk, dict) and chunk.get('body'):
                        body = json.loads(chunk['body'])
                        if pr_details := body.get('pr_details'):
                            return pr_details
            elif isinstance(self.event_data, dict):
                # ë‹¨ì¼ ì²˜ë¦¬ ê²°ê³¼
                if self.event_data.get('body'):
                    body = json.loads(self.event_data['body'])
                    if pr_details := body.get('pr_details'):
                        return pr_details

            return {}
        except Exception as e:
            print(f"Error extracting PR details: {e}")
            return {}

    def _extract_chunk_results(self) -> List[Dict[str, Any]]:
        """ì²­í¬ ê²°ê³¼ ì¶”ì¶œ"""
        results = []
        try:
            if isinstance(self.event_data, list):
                # ë³‘ë ¬ ì²˜ë¦¬ ê²°ê³¼
                for chunk in self.event_data:
                    if isinstance(chunk, dict) and chunk.get('body'):
                        body = json.loads(chunk['body'])
                        if chunk_results := body.get('results'):
                            results.extend(chunk_results)
            elif isinstance(self.event_data, dict):
                # ë‹¨ì¼ ì²˜ë¦¬ ê²°ê³¼
                if self.event_data.get('body'):
                    body = json.loads(self.event_data['body'])
                    if chunk_results := body.get('results'):
                        results.extend(chunk_results)
        except Exception as e:
            print(f"Error extracting chunk results: {e}")

        return results

    def _normalize_line_number(self, line_number: Union[str, int]) -> str:
        """ë¼ì¸ ë²ˆí˜¸ ì •ê·œí™”"""
        if isinstance(line_number, str) and line_number.lower() == 'all':
            return 'Throughout file'
        try:
            return str(int(line_number))
        except (ValueError, TypeError):
            return 'N/A'


    def _prepare_summary_prompt(self, changes: Dict[str, List[str]]) -> str:
        """Key Changes Summary ìš”ì•½ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„"""
        prompt = """ë‹¤ìŒ ë³€ê²½ì‚¬í•­ë“¤ì„ ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ 5ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
        ì›ë³¸ ë³€ê²½ì‚¬í•­:

        ğŸ”„ Functional Changes:
        """
        for change in changes.get('functional_changes', []):
            prompt += f"- {change}\n"

        prompt += "\nğŸ— Architectural Changes:\n"
        for change in changes.get('architectural_changes', []):
            prompt += f"- {change}\n"

        prompt += "\nğŸ”§ Technical Improvements:\n"
        for change in changes.get('technical_improvements', []):
            prompt += f"- {change}\n"

        prompt += """
        ìœ„ ë³€ê²½ì‚¬í•­ë“¤ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

            {
                "summary": {
                    "functional_changes": "2ë¬¸ì¥ ì´ë‚´ì˜ ê¸°ëŠ¥ì  ë³€ê²½ì‚¬í•­ ìš”ì•½",
                    "architectural_changes": "2ë¬¸ì¥ ì´ë‚´ì˜ ì•„í‚¤í…ì²˜ ë³€ê²½ì‚¬í•­ ìš”ì•½",
                    "technical_improvements": "2ë¬¸ì¥ ì´ë‚´ì˜ ê¸°ìˆ ì  ê°œì„ ì‚¬í•­ ìš”ì•½"
                }
            }

            ê° ìš”ì•½ì€ í•œê¸€ë¡œ ì‘ì„±í•˜ê³ , ì „ë¬¸ ìš©ì–´ë‚˜ ê³ ìœ ëª…ì‚¬ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì£¼ì„¸ìš”."""
        print(prompt)
        return prompt

    def _summarize_changes_with_bedrock(self, changes: Dict[str, List[str]]) -> Dict[str, str]:
        """Bedrockì„ ì‚¬ìš©í•˜ì—¬ ë³€ê²½ì‚¬í•­ ìš”ì•½"""
        try:
            bedrock = boto3.client('bedrock-runtime')
            prompt = self._prepare_summary_prompt(changes)

            body = json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1000,
                "temperature": 0.7,
                "top_p": 0.9,
                "system": "2ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ ë¦¬ë·°ì–´ì…ë‹ˆë‹¤.",
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            })

            response = bedrock.invoke_model(
                modelId=self.config['model'],
                contentType='application/json',
                accept='application/json',
                body=body.encode()
            )

            response_body = json.loads(response['body'].read())
            summary = json.loads(response_body['content'][0]['text'])
            return summary.get('summary', {})

        except Exception as e:
            print(f"Error summarizing with Bedrock: {e}")
            return {
                'functional_changes': '',
                'architectural_changes': '',
                'technical_improvements': ''
            }

    def analyze_results(self) -> ReviewSummary:
        """ë¦¬ë·° ê²°ê³¼ ë¶„ì„"""
        severity_counts = defaultdict(int)
        category_counts = defaultdict(int)
        critical_issues = []
        major_issues = []
        suggestions_by_file = defaultdict(list)
        reference_context = defaultdict(list)
        total_issues = 0

        # primary/reference íŒŒì¼ êµ¬ë¶„
        primary_files = []
        reference_files = []

        for result in self.chunk_results:
            file_path = result['file_path']
            
            if result.get('is_primary', True):
                primary_files.append(file_path)
                severity_counts[result['severity']] += 1
                
                # ì°¸ì¡° íŒŒì¼ ì •ë³´ ì €ì¥
                if referenced_by := result.get('referenced_by'):
                    reference_context[file_path].extend(referenced_by)
                
                for suggestion in result.get('suggestions', []):
                    total_issues += 1
                    category = suggestion.get('category', 'other')
                    severity = suggestion.get('severity', 'NORMAL')
                    
                    category_counts[category] += 1
                    
                    # ë¼ì¸ ë²ˆí˜¸ ì •ê·œí™”
                    suggestion['line_number'] = self._normalize_line_number(
                        suggestion.get('line_number')
                    )
                    
                    issue_details = {
                        'file': file_path,
                        'description': suggestion.get('description'),
                        'line_number': suggestion['line_number'],
                        'suggestion': suggestion.get('suggestion')
                    }
                    
                    if severity == 'CRITICAL':
                        critical_issues.append(issue_details)
                    elif severity == 'MAJOR':
                        major_issues.append(issue_details)
                    
                    suggestions_by_file[file_path].append(suggestion)
            else:
                reference_files.append(file_path)

        # ë³€ê²½ì‚¬í•­ ìš”ì•½ ìˆ˜ì§‘
        functional_changes = set()
        architectural_changes = set()
        technical_improvements = set()

        for result in self.chunk_results:
            if summary := result.get('summary', {}):
                functional_changes.update(summary.get('functional_changes', []))
                architectural_changes.update(summary.get('architectural_changes', []))
                technical_improvements.update(summary.get('technical_improvements', []))

        return ReviewSummary(
            total_files=len(primary_files) + len(reference_files),
            total_primary_files=len(primary_files),
            total_reference_files=len(reference_files),
            total_issues=total_issues,
            severity_counts=dict(severity_counts),
            category_counts=dict(category_counts),
            critical_issues=critical_issues,
            major_issues=major_issues,
            suggestions_by_file=dict(suggestions_by_file),
            reference_context=dict(reference_context),
            functional_changes=sorted(list(functional_changes)),
            architectural_changes=sorted(list(architectural_changes)),
            technical_improvements=sorted(list(technical_improvements))
        )

    def generate_markdown_report(self, summary: ReviewSummary) -> str:
        pr_title = self.pr_details.get('title', 'Unknown PR')
        pr_author = self.pr_details.get('author', 'Unknown Author')

        report = [
            f"# ğŸ§¾ Code Review Report: {pr_title}",
            f"\nGenerated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",

            "\n## Overview",
            f"- Pull Request by: {pr_author}",
            f"- Primary Files Reviewed: {summary.total_primary_files}",
            f"- Reference Files: {summary.total_reference_files}",
            f"- Total Issues Found: {summary.total_issues}",
        ]

        if summary.functional_changes or summary.architectural_changes or summary.technical_improvements:
            # ëª¨ë“  ë³€ê²½ì‚¬í•­ í†µí•©
            all_changes = {
                'functional_changes': summary.functional_changes,
                'architectural_changes': summary.architectural_changes,
                'technical_improvements': summary.technical_improvements
            }
        
            # Bedrockì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½
            summarized_changes = self._summarize_changes_with_bedrock(all_changes)

            report.extend([
                "\n## Key Changes Summary",
                "\n### ğŸ”„ Functional Changes",
                summarized_changes.get('functional_changes', ''),
                "\n### ğŸ— Architectural Changes",
                summarized_changes.get('architectural_changes', ''),
                "\n### ğŸ”§ Technical Improvements",
                summarized_changes.get('technical_improvements', '')
            ])

        report.extend([
            "\n## Severity Summary",
            "| Severity | Count |",
            "|----------|-------|"
        ])

        # ì‹¬ê°ë„ ìš”ì•½ í…Œì´ë¸”
        for severity, count in sorted(summary.severity_counts.items()):
            report.append(f"| {severity} | {count} |")

        # ì¹´í…Œê³ ë¦¬ ìš”ì•½ í…Œì´ë¸”    
        report.extend([
            "\n## Category Summary",
            "| Category | Count |",
            "|----------|-------|"
        ])

        for category, count in sorted(summary.category_counts.items()):
            report.append(f"| {category.title()} | {count} |")

        # ì¤‘ìš” ì´ìŠˆ ì„¹ì…˜
        if summary.critical_issues:
            report.append("\n## Critical Issues")
            for issue in summary.critical_issues:
                report.extend([
                    f"\n### {issue['file']} (Line {issue['line_number']})",
                    f"**Issue:** {issue['description']}",
                    f"**Suggestion:** {issue['suggestion']}"
                ])

        if summary.major_issues:
            report.append("\n## Major Issues")
            for issue in summary.major_issues:
                report.extend([
                    f"\n### {issue['file']} (Line {issue['line_number']})",
                    f"**Issue:** {issue['description']}",
                    f"**Suggestion:** {issue['suggestion']}"
                ])

        # íŒŒì¼ë³„ ìƒì„¸ ë¦¬ë·°
        report.append("\n## Detailed Review by File")
        
        # ëª¨ë“  ì´ìŠˆë¥¼ í•˜ë‚˜ì˜ í…Œì´ë¸”ë¡œ í†µí•©
        report.extend([
            "\n| File | Line | Category | Severity | Description | Suggestion |",
            "|------|------|-----------|-----------|--------------|-------------|"
        ])

        # ëª¨ë“  íŒŒì¼ì˜ ì œì•ˆì‚¬í•­ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©
        all_suggestions = []
        for file_path, suggestions in summary.suggestions_by_file.items():
            for suggestion in suggestions:
                all_suggestions.append((file_path, suggestion))

        # íŒŒì¼ëª…ê³¼ ë¼ì¸ ë²ˆí˜¸ë¡œ ì •ë ¬
        sorted_suggestions = sorted(
            all_suggestions,
            key=lambda x: (
                x[0],  # íŒŒì¼ëª…ìœ¼ë¡œ ë¨¼ì € ì •ë ¬
                # 'Throughout file'ë¥¼ ë§ˆì§€ë§‰ìœ¼ë¡œ
                x[1]['line_number'] == 'Throughout file',
                # ìˆ«ìëŠ” ìˆ«ììˆœìœ¼ë¡œ
                int(x[1]['line_number']) if x[1]['line_number'].isdigit() else float('inf'),
                # ë‚˜ë¨¸ì§€ëŠ” ë¬¸ìì—´ ìˆœìœ¼ë¡œ
                x[1]['line_number']
            )
        )

        # í…Œì´ë¸” ìƒì„±
        for file_path, suggestion in sorted_suggestions:
            # ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ì—ì„œ íŒŒì´í”„(|) ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
            description = suggestion.get('description', 'N/A').replace('|', '\\|')
            suggestion_text = suggestion.get('suggestion', 'N/A').replace('|', '\\|')

            report.append(
                f"| {file_path} | {suggestion['line_number']} | "
                f"{suggestion.get('category', 'Other').title()} | "
                f"{suggestion.get('severity', 'NORMAL')} | "
                f"{description} | "
                f"{suggestion_text} |"
            )

        # íŒŒì¼ ì˜ì¡´ì„± ì •ë³´ë¥¼ ë³„ë„ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¦¬
        report.append("\n### File Dependencies")
        for file_path, ref_files in sorted(summary.reference_context.items()):
            if ref_files:  # ì°¸ì¡° íŒŒì¼ì´ ìˆëŠ” ê²½ìš°ë§Œ í‘œì‹œ
                report.extend([
                    f"\n#### {file_path}",
                    "Related Files:"
                ])
                dedup_ref_files = list(set(ref_files))
                for ref_file in sorted(dedup_ref_files):
                    report.append(f"- {ref_file}")

        # ì¶”ê°€ ì •ë³´ ë° ë©”íƒ€ë°ì´í„°
        report.extend([
            "\n## Additional Information",
            "- Review Date: " + datetime.now().strftime('%Y-%m-%d'),
            "- Base Branch: " + self.pr_details.get('base_branch', 'Unknown'),
            "- Head Branch: " + self.pr_details.get('head_branch', 'Unknown'),
            f"- Repository: {self.pr_details.get('repository', 'Unknown')}",
            f"- PR Number: {self.pr_details.get('pr_id', 'Unknown')}"
        ])

        # ë¦¬í¬íŠ¸ í•˜ë‹¨ì— ìë™ ìƒì„± í‘œì‹œ
        report.extend([
            "\n---",
            "ğŸ¤– _This report was automatically generated by PR Review Bot & Amazon Bedrock_ ğŸ§¾"
        ])

        return '\n'.join(report)

    def prepare_pr_comment(self, summary: ReviewSummary) -> str:
        """PR ì½”ë©˜íŠ¸ìš© ìš”ì•½ ìƒì„±"""
        comment = [
            "# Code Review Summary",
            f"\nReviewed {summary.total_primary_files} primary files "
            f"(with {summary.total_reference_files} reference files) "
            f"and found {summary.total_issues} issues.",
            
            "\n## Severity Breakdown",
            "| Severity | Count |",
            "|----------|-------|",
        ]
        
        for severity, count in summary.severity_counts.items():
            comment.append(f"| {severity} | {count} |")
        
        if summary.critical_issues:
            comment.append("\n### Critical Issues Found")
            for issue in summary.critical_issues:
                comment.extend([
                    f"\n- **{issue['file']}** (Line {issue['line_number']})",
                    f"  - {issue['description']}",
                    f"  - Suggestion: {issue['suggestion']}"
                ])
        
        if summary.major_issues:
            comment.append("\n### Major Issues Found")
            for issue in summary.major_issues[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                comment.extend([
                    f"\n- **{issue['file']}** (Line {issue['line_number']})",
                    f"  - {issue['description']}"
                ])
            
            if len(summary.major_issues) > 5:
                comment.append(f"\n... and {len(summary.major_issues) - 5} more major issues.")
        
        return '\n'.join(comment)

    def prepare_slack_message(self, summary: ReviewSummary) -> Dict[str, Any]:
        """Slack ë©”ì‹œì§€ ì¤€ë¹„"""
        pr_title = self.pr_details.get('title', 'Unknown PR')
        pr_author = self.pr_details.get('author', 'Unknown Author')
        pr_url = self.pr_details.get('pr_url', '#')

        # PR ì œëª©ì´ ê¸¸ ê²½ìš° ì¶•ì•½
        MAX_TITLE_LENGTH = 100
        shortened_title = (pr_title[:MAX_TITLE_LENGTH] + '...') if len(pr_title) > MAX_TITLE_LENGTH else pr_title
        
        severity_emoji = {
            'CRITICAL': 'ğŸš¨',
            'MAJOR': 'âš ï¸',
            'MINOR': 'ğŸ“',
            'NORMAL': 'âœ…'
        }
        
        # ì „ì²´ ì‹¬ê°ë„ ê²°ì •
        overall_severity = 'NORMAL'
        if summary.critical_issues:
            overall_severity = 'CRITICAL'
        elif summary.major_issues:
            overall_severity = 'MAJOR'
        elif summary.severity_counts.get('MINOR', 0) > 0:
            overall_severity = 'MINOR'
        
        blocks = [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": f"{severity_emoji[overall_severity]} Review: {shortened_title}"
                }
            },
            {
                "type": "section",
                "fields": [
                    {
                        "type": "mrkdwn",
                        "text": f"*Author:*\n{pr_author}"
                    },
                    {
                        "type": "mrkdwn",
                        "text": f"*Files:*\n{summary.total_primary_files} primary + {summary.total_reference_files} reference"
                    }
                ]
            }
        ]
        
        # ì‹¬ê°ë„ ìš”ì•½
        severity_text = []
        for severity, count in summary.severity_counts.items():
            if count > 0:
                severity_text.append(f"{severity_emoji[severity]} {severity}: {count}")
        
        blocks.append({
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": "\n".join(severity_text)
            }
        })
        
        # ì¤‘ìš” ì´ìŠˆ í•˜ì´ë¼ì´íŠ¸
        if summary.critical_issues or summary.major_issues:
            highlight_text = ["*Critical/Major Issues:*"]
            
            for issue in (summary.critical_issues + summary.major_issues)[:3]:
                highlight_text.append(
                    f"â€¢ {issue['file']} (Line {issue['line_number']}): {issue['description'][:100]}..."
                )
            
            if len(summary.critical_issues + summary.major_issues) > 3:
                remaining = len(summary.critical_issues + summary.major_issues) - 3
                highlight_text.append(f"_...and {remaining} more critical/major issues_")
            
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": "\n".join(highlight_text)
                }
            })
        
        # íŒŒì¼ í†µê³„ ì„¹ì…˜
        if summary.reference_context:
            file_stats = ["*File Dependencies:*"]
            for primary_file, ref_files in list(summary.reference_context.items())[:3]:
                file_stats.append(f"â€¢ `{primary_file}` - {len(ref_files)} related files")
            
            if len(summary.reference_context) > 3:
                remaining = len(summary.reference_context) - 3
                file_stats.append(f"_...and {remaining} more files with dependencies_")
            
            blocks.append({
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": "\n".join(file_stats)
                }
            })
        
        # PR ë§í¬ ë²„íŠ¼
        if pr_url and pr_url != '#':
            blocks.append({
                "type": "actions",
                "elements": [
                    {
                        "type": "button",
                        "text": {
                            "type": "plain_text",
                            "text": "Review PR ğŸ‘€"
                        },
                        "url": pr_url,
                        "style": "primary"
                    }
                ]
            })
        
        return {
            "blocks": blocks,
            "text": f"Code Review completed for PR: {shortened_title} - Found {summary.total_issues} issues in {summary.total_primary_files} primary files"  # í´ë°± í…ìŠ¤íŠ¸
        }

def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """Lambda í•¸ë“¤ëŸ¬"""
    try:

         # ì‹¤íŒ¨ ì²­í¬ ì¬ì‹œë„ ê²°ê³¼ ì²˜ë¦¬
        classified_results = event.get('classifiedResults', {})
        succeeded_results = classified_results.get('succeeded', [])
        retry_results = event.get('retryResults', [])

        # ëª¨ë“  ê²°ê³¼ ë³‘í•© (ì¬ì‹œë„ ê²°ê³¼ë¥¼ ì„±ê³µ ê²°ê³¼ì— ì¶”ê°€)
        all_results = succeeded_results + retry_results

        # ê²°ê³¼ ì§‘ê³„ê¸° ì´ˆê¸°í™” - eventë¥¼ ì§ì ‘ ì „ë‹¬
        aggregator = ResultAggregator(all_results)
        summary = aggregator.analyze_results()
        
        markdown_report = aggregator.generate_markdown_report(summary)
        pr_comment = aggregator.prepare_pr_comment(summary)
        slack_message = aggregator.prepare_slack_message(summary)
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'summary': {
                    'total_files': summary.total_files,
                    'total_primary_files': summary.total_primary_files,
                    'total_reference_files': summary.total_reference_files,
                    'total_issues': summary.total_issues,
                    'severity_counts': summary.severity_counts,
                    'category_counts': summary.category_counts
                },
                'markdown_report': markdown_report,
                'pr_comment': pr_comment,
                'slack_message': slack_message,
                'pr_details': aggregator.pr_details,
                'reference_context': summary.reference_context
            }, ensure_ascii=False)
        }
        
    except Exception as e:
        print(f"Error aggregating results: {e}")
        return {
            'statusCode': 500,
            'body': json.dumps({
                'error': str(e)
            })
        }